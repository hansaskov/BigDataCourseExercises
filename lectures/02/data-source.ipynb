{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass, field, asdict\n",
    "from datetime import datetime\n",
    "from uuid import uuid4\n",
    "\n",
    "def get_uuid():\n",
    "    return str(uuid4())\n",
    "\n",
    "@dataclass\n",
    "class SensorObj:\n",
    "    sensor_id: str\n",
    "    modality: float\n",
    "    unit: str\n",
    "    temporal_aspect: str\n",
    "\n",
    "    def to_dict(self) -> dict:\n",
    "        return asdict(self)\n",
    "\n",
    "\n",
    "import json \n",
    "@dataclass\n",
    "class PackageObj:\n",
    "    payload: SensorObj\n",
    "    correlation_id: str = field(default_factory=get_uuid)\n",
    "    created_at: datetime = field(default_factory=datetime.utcnow)\n",
    "    schema_version: int = field(default=1)\n",
    "\n",
    "    def to_dict(self):\n",
    "        self.created_at = self.created_at.timestamp()\n",
    "        self.payload = json.dumps(self.payload.to_dict())\n",
    "        return asdict(self)\n",
    "    \n",
    "VALID_SENSOR_IDS: list[int] = [1,2,3,4,5,6]\n",
    "VALID_TEMPORAL_ASPECTS: list[str] = [\"real_time\", \"edge_prediction\"]\n",
    "VALID_RANGE:tuple[int] = (-600, 600)\n",
    "\n",
    "SCHEMA = {\n",
    "\"type\": \"record\",\n",
    "\"namespace\": \"default\",\n",
    "\"name\": \"SENSORPACKAGES\",\n",
    "\"fields\": [\n",
    "    {\n",
    "    \"name\": \"payload\",\n",
    "    \"doc\": \"Payload of the message.\",\n",
    "    \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "    \"name\": \"correlation_id\",\n",
    "    \"doc\": \"UUID of this message.\",\n",
    "    \"type\": \"string\"\n",
    "    },\n",
    "    {\n",
    "    \"name\": \"created_at\",\n",
    "    \"doc\": \"Timestamp (UTC) for msg creation.\",\n",
    "    \"type\": \"double\"\n",
    "    },\n",
    "    {\n",
    "    \"name\": \"schema_version\",\n",
    "    \"doc\": \"Integer verion number of the msg schema.\",\n",
    "    \"type\": \"int\"\n",
    "    },\n",
    "]\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "def get_sensor_sample(sensor_id:int = None, modality:int = None, unit: str = \"MW\", temporal_aspect: str=VALID_TEMPORAL_ASPECTS[0]) -> SensorObj:\n",
    "    \n",
    "    if sensor_id is None:\n",
    "        sensor_id = random.choice(VALID_SENSOR_IDS)\n",
    "    if modality is None:\n",
    "        modality = random.choice(range(VALID_RANGE[0],VALID_RANGE[1]+1))\n",
    "    return SensorObj(sensor_id=sensor_id, modality=modality, unit=unit, temporal_aspect=temporal_aspect)\n",
    "\n",
    "po = PackageObj(payload=get_sensor_sample(sensor_id = 1))\n",
    "po.to_dict()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from hdfs.ext.avro import AvroWriter\n",
    "from client import InsecureClient\n",
    "\n",
    "def get_filename(self, format: str = \"avro\") -> str:\n",
    "    return f\"/data/raw/sensor_id={self.payload.sensor_id}/temporal_aspect={self.payload.temporal_aspect}/{self.created_at.strftime('year=%Y/month=%m/day=%d')}/{self.correlation_id}.{format}\"\n",
    "\n",
    "def generate_sample(sensor_id:str, hdfs_client: InsecureClient) -> None:\n",
    "    po = PackageObj(payload=get_sensor_sample(sensor_id=sensor_id))\n",
    "    filename:str = get_filename(po)\n",
    "    print(po)\n",
    "    with AvroWriter(client = hdfs_client, hdfs_path = filename, schema=SCHEMA, overwrite=True) as writer:\n",
    "        writer.write(po.to_dict())\n",
    "\n",
    "generate_sample(1, hdfs_client)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import threading\n",
    "from client import get_hdfs_client\n",
    "\n",
    "hdfs_client = get_hdfs_client()\n",
    "\n",
    "class RepeatTimer(threading.Timer):  \n",
    "    def run(self):  \n",
    "        while not self.finished.wait(self.interval):  \n",
    "            self.function(*self.args, **self.kwargs)\n",
    "\n",
    "timer1 = RepeatTimer(1.0, generate_sample, [1, hdfs_client])\n",
    "timer2 = RepeatTimer(1.0, generate_sample, [2, hdfs_client])\n",
    "timer3 = RepeatTimer(1.0, generate_sample, [3, hdfs_client])\n",
    "timer4 = RepeatTimer(1.0, generate_sample, [4, hdfs_client])\n",
    "timer5 = RepeatTimer(1.0, generate_sample, [5, hdfs_client])\n",
    "timer6 = RepeatTimer(1.0, generate_sample, [6, hdfs_client])\n",
    "\n",
    "timer1.start()\n",
    "timer2.start()\n",
    "timer3.start()\n",
    "timer4.start()\n",
    "timer5.start()\n",
    "timer6.start()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "timer1.cancel()\n",
    "timer2.cancel()\n",
    "timer3.cancel()\n",
    "timer4.cancel()\n",
    "timer5.cancel()\n",
    "timer6.cancel()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from hdfs.ext.avro import AvroReader\n",
    "\n",
    "# filename: str = None\n",
    "# with AvroReader(hdfs_client, filename) as reader:\n",
    "#     schema = reader.schema  # The inferred schema.\n",
    "#     content = reader.content  # The remote file's HDFS content object.\n",
    "#     print(schema)\n",
    "#     print(content)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
